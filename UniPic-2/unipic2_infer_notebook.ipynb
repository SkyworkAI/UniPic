{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3017de32-377c-44a6-ae97-3ebb84e009b6",
   "metadata": {},
   "source": [
    "# SD3.5M Kontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1054c815-306b-4144-90cd-6c99772d64f0",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-12T03:09:06.716877Z",
     "iopub.status.busy": "2025-08-12T03:09:06.716467Z",
     "iopub.status.idle": "2025-08-12T03:09:32.646409Z",
     "shell.execute_reply": "2025-08-12T03:09:32.645716Z",
     "shell.execute_reply.started": "2025-08-12T03:09:06.716848Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e2332f15bb46e0a3ce57f46a84e039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/usr/local/lib/python3.10/site-packages/apex/normalization/fused_layer_norm.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879952c12c4a4ae1ad5a97404c3a40f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from unipicv2.pipeline_stable_diffusion_3_kontext import StableDiffusion3KontextPipeline\n",
    "from unipicv2.transformer_sd3_kontext import SD3Transformer2DKontextModel\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n",
    "\n",
    "# Load model components\n",
    "pretrained_model_name_or_path = \"/ckpt/path/...\"\n",
    "\n",
    "transformer = SD3Transformer2DKontextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"transformer\", torch_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\", torch_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "# Load text encoders\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16).cuda()\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "\n",
    "text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder_2\", torch_dtype=torch.bfloat16).cuda()\n",
    "tokenizer_2 = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer_2\")\n",
    "\n",
    "text_encoder_3 = T5EncoderModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder_3\", torch_dtype=torch.bfloat16).cuda()\n",
    "tokenizer_3 = T5TokenizerFast.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer_3\")\n",
    "\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = StableDiffusion3KontextPipeline(\n",
    "    transformer=transformer, vae=vae,\n",
    "    text_encoder=text_encoder, tokenizer=tokenizer,\n",
    "    text_encoder_2=text_encoder_2, tokenizer_2=tokenizer_2,\n",
    "    text_encoder_3=text_encoder_3, tokenizer_3=tokenizer_3,\n",
    "    scheduler=scheduler)\n",
    "\n",
    "# Generate image\n",
    "image = pipeline(\n",
    "    prompt='a pig with wings and a top hat flying over a happy futuristic scifi city',\n",
    "    negative_prompt='',\n",
    "    height=512, width=384,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=3.5,\n",
    "    generator=torch.Generator(device=transformer.device).manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "image.save(\"text2image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f190a24d-dd83-4b45-a2cc-031b53c9c3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T03:10:04.487896Z",
     "iopub.status.busy": "2025-08-12T03:10:04.487278Z",
     "iopub.status.idle": "2025-08-12T03:10:07.047809Z",
     "shell.execute_reply": "2025-08-12T03:10:07.047218Z",
     "shell.execute_reply.started": "2025-08-12T03:10:04.487869Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b792ca106a15481a8e01bf39f845b819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess image\n",
    "def fix_longer_edge(x, image_size, factor=32):\n",
    "    w, h = x.size\n",
    "    if w >= h:\n",
    "        target_w = image_size\n",
    "        target_h = h * (target_w / w)\n",
    "        target_h = round(target_h / factor) * factor\n",
    "    else:\n",
    "        target_h = image_size\n",
    "        target_w = w * (target_h / h)\n",
    "        target_w = round(target_w / factor) * factor\n",
    "    x = x.resize(size=(target_w, target_h))\n",
    "    return x\n",
    "\n",
    "image = Image.open(\"text2image.png\")\n",
    "image = fix_longer_edge(image, image_size=512)\n",
    "\n",
    "negative_prompt = \"blurry, low quality, low resolution, distorted, deformed, broken content, missing parts, damaged details, artifacts, glitch, noise, pixelated, grainy, compression artifacts, bad composition, wrong proportion, incomplete editing, unfinished, unedited areas.\"\n",
    "\n",
    "# Edit image\n",
    "edited_image = pipeline(\n",
    "    image=image,\n",
    "    prompt=\"remove the pig's hat\",\n",
    "    negative_prompt=negative_prompt,\n",
    "    height=image.height, width=image.width,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=3.5,\n",
    "    generator=torch.Generator(device=transformer.device).manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "edited_image.save(\"image_editing.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6016d-81c8-4a5d-ad53-6310cb1d1046",
   "metadata": {},
   "source": [
    "# Qwen2.5-VL + SD3.5M Kontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06788413-83ec-4bd3-8f30-1777750e362f",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-08-12T03:19:49.072883Z",
     "iopub.status.busy": "2025-08-12T03:19:49.072699Z",
     "iopub.status.idle": "2025-08-12T03:20:08.113461Z",
     "shell.execute_reply": "2025-08-12T03:20:08.112776Z",
     "shell.execute_reply.started": "2025-08-12T03:19:49.072858Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e168737195284accbec31bf67819d66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22a8296644344c994ce4e34e9ed4440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from unipicv2.pipeline_stable_diffusion_3_kontext import StableDiffusion3KontextPipeline\n",
    "from unipicv2.transformer_sd3_kontext import SD3Transformer2DKontextModel\n",
    "from unipicv2.stable_diffusion_3_conditioner import StableDiffusion3Conditioner\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL\n",
    "\n",
    "# Load model components\n",
    "pretrained_model_name_or_path = \"/ckpt/path/...\"\n",
    "\n",
    "transformer = SD3Transformer2DKontextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"transformer\", torch_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\", torch_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "# Load Qwen2.5-VL model\n",
    "lmm = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\").cuda()\n",
    "\n",
    "processor = Qwen2_5_VLProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "processor.chat_template = processor.chat_template.replace(\n",
    "    \"{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}\",\n",
    "    \"\")\n",
    "\n",
    "conditioner = StableDiffusion3Conditioner.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"conditioner\", torch_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "# Create pipeline (note: text encoders set to None)\n",
    "pipeline = StableDiffusion3KontextPipeline(\n",
    "    transformer=transformer, vae=vae,\n",
    "    text_encoder=None, tokenizer=None,\n",
    "    text_encoder_2=None, tokenizer_2=None,\n",
    "    text_encoder_3=None, tokenizer_3=None,\n",
    "    scheduler=scheduler)\n",
    "\n",
    "# Prepare prompts\n",
    "prompt = 'a pig with wings and a top hat flying over a happy futuristic scifi city'\n",
    "negative_prompt = ''\n",
    "\n",
    "messages = [[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f'Generate an image: {txt}'}]}]\n",
    "            for txt in [prompt, negative_prompt]]\n",
    "\n",
    "texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "inputs = processor(text=texts, images=None, videos=None, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Process with Qwen2.5-VL\n",
    "input_ids, attention_mask = inputs.input_ids, inputs.attention_mask\n",
    "input_ids = torch.cat([input_ids, input_ids.new_zeros(2, conditioner.config.num_queries)], dim=1)\n",
    "attention_mask = torch.cat([attention_mask, attention_mask.new_ones(2, conditioner.config.num_queries)], dim=1)\n",
    "inputs_embeds = lmm.get_input_embeddings()(input_ids)\n",
    "inputs_embeds[:, -conditioner.config.num_queries:] = conditioner.meta_queries[None].expand(2, -1, -1)\n",
    "\n",
    "outputs = lmm.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, use_cache=False)\n",
    "hidden_states = outputs.last_hidden_state[:, -conditioner.config.num_queries:]\n",
    "prompt_embeds, pooled_prompt_embeds = conditioner(hidden_states)\n",
    "\n",
    "# Generate image\n",
    "image = pipeline(\n",
    "    prompt_embeds=prompt_embeds[:1],\n",
    "    pooled_prompt_embeds=pooled_prompt_embeds[:1],\n",
    "    negative_prompt_embeds=prompt_embeds[1:],\n",
    "    negative_pooled_prompt_embeds=pooled_prompt_embeds[1:],\n",
    "    height=512, width=384,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=3.5,\n",
    "    generator=torch.Generator(device=transformer.device).manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "image.save(\"text2image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9b0fd4-9041-4946-ad3d-bf2c102505b0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-12T03:20:08.114992Z",
     "iopub.status.busy": "2025-08-12T03:20:08.114524Z",
     "iopub.status.idle": "2025-08-12T03:20:10.722211Z",
     "shell.execute_reply": "2025-08-12T03:20:10.721513Z",
     "shell.execute_reply.started": "2025-08-12T03:20:08.114970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858bd81715ad438794cc06505fe0c426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess image\n",
    "def fix_longer_edge(x, image_size, factor=32):\n",
    "    w, h = x.size\n",
    "    if w >= h:\n",
    "        target_w = image_size\n",
    "        target_h = h * (target_w / w)\n",
    "        target_h = round(target_h / factor) * factor\n",
    "    else:\n",
    "        target_h = image_size\n",
    "        target_w = w * (target_h / h)\n",
    "        target_w = round(target_w / factor) * factor\n",
    "    x = x.resize(size=(target_w, target_h))\n",
    "    return x\n",
    "\n",
    "# Load image for editing\n",
    "image = Image.open(\"text2image.png\")\n",
    "image = fix_longer_edge(image, image_size=512)\n",
    "\n",
    "prompt = \"remove the pig's hat\"\n",
    "negative_prompt = \"blurry, low quality, low resolution, distorted, deformed, broken content, missing parts, damaged details, artifacts, glitch, noise, pixelated, grainy, compression artifacts, bad composition, wrong proportion, incomplete editing, unfinished, unedited areas.\"\n",
    "\n",
    "# Prepare messages with image input\n",
    "messages = [[{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": txt}]}]\n",
    "            for txt in [prompt, negative_prompt]]\n",
    "\n",
    "texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "\n",
    "min_pixels = max_pixels = int(image.height * 28 / 32 * image.width * 28 / 32)\n",
    "inputs = processor(\n",
    "    text=texts, images=[image]*2,\n",
    "    min_pixels=min_pixels, max_pixels=max_pixels,\n",
    "    videos=None, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Process with vision understanding\n",
    "input_ids, attention_mask, pixel_values, image_grid_thw = \\\n",
    "    inputs.input_ids, inputs.attention_mask, inputs.pixel_values, inputs.image_grid_thw\n",
    "\n",
    "input_ids = torch.cat([input_ids, input_ids.new_zeros(2, conditioner.config.num_queries)], dim=1)\n",
    "attention_mask = torch.cat([attention_mask, attention_mask.new_ones(2, conditioner.config.num_queries)], dim=1)\n",
    "inputs_embeds = lmm.get_input_embeddings()(input_ids)\n",
    "inputs_embeds[:, -conditioner.config.num_queries:] = conditioner.meta_queries[None].expand(2, -1, -1)\n",
    "\n",
    "image_embeds = lmm.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "image_token_id = processor.tokenizer.convert_tokens_to_ids('<|image_pad|>')\n",
    "inputs_embeds[input_ids == image_token_id] = image_embeds\n",
    "\n",
    "lmm.model.rope_deltas = None\n",
    "outputs = lmm.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask,\n",
    "                    image_grid_thw=image_grid_thw, use_cache=False)\n",
    "\n",
    "hidden_states = outputs.last_hidden_state[:, -conditioner.config.num_queries:]\n",
    "prompt_embeds, pooled_prompt_embeds = conditioner(hidden_states)\n",
    "\n",
    "# Generate edited image\n",
    "edited_image = pipeline(\n",
    "    image=image,\n",
    "    prompt_embeds=prompt_embeds[:1],\n",
    "    pooled_prompt_embeds=pooled_prompt_embeds[:1],\n",
    "    negative_prompt_embeds=prompt_embeds[1:],\n",
    "    negative_pooled_prompt_embeds=pooled_prompt_embeds[1:],\n",
    "    height=image.height, width=image.width,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=3.5,\n",
    "    generator=torch.Generator(device=transformer.device).manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "edited_image.save(\"image_editing.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35652b-3573-4f37-973f-7e06f2b34560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa67de-7532-4907-856b-6044592e29ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
